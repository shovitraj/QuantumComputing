{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The roots of probabilistic graphical models go back to the 1980s, with a strong connection to Bayesian statistics. The story resembles that of neural networks: they have been around for over three decades and they need massive computational power. However, unlike in the case of deep learning, the requirements for computational resources remain out of reach. These models require sampling a distribution, and very often it is the Boltzmann distribution. Since quantum computers can give samples from this distribution, we can hope that quantum hardware can enable these models the same way graphics processing units enabled deep learning.\n",
    "\n",
    "# Probabilistic graphical models\n",
    "\n",
    "Probabilistic graphical models capture a compact representation of a joint probability distribution. For $\\{X_1,\\ldots,X_N\\}$ binary random variables, there are $2^N$ assignments. In a graphical model, complexity is dealt with through graph theory. We get both an efficient treatment of uncertainty (probabilities) and of logical structure (independence constraints). The factorization of the probabilities happens along conditional independences among random variables. The definition is that $X$ is conditionally independent of $Y$ given $Z$ $(X\\perp Y|Z)$, if $P(X=x, Y=y|Z=z) = P(X=x|Z=z)P(Y=y|Z=z)$ for all $x\\in X,y\\in Y,z\\in Z$.\n",
    "\n",
    "The graph can be directed -- these are called Bayesian networks in general -- or undirected, in the case of Markov networks (also known as Markov random fields) [[1](#1)]. Graphical models are quintessentially generative: we explicitly model a probability distribution. Thus generating new samples is trivial and we can always introduce extra random variables to ensure certain properties. These models also take us a step closer to explainability, either by the use of the random variables directly for explanations (if your model is such) or by introducing explanatory random variables that correlate with the others.\n",
    "\n",
    "In a Markov random field, we can allow cycles in the graph and switch from local normalization (conditional probability distribution at each node) to global normalization of probabilities (i.e. a partition function). Examples include countless applications in computer vision, pattern recognition, artificial intelligence, but also Ising models that we have seen before: the factors are defined as degree-1 and degree-2 monomials of the random variables connected in the graph.\n",
    "\n",
    "The factorization is given as a sum $P(X_1, \\ldots, X_N) = \\frac{1}{Z}\\exp(-\\sum_k E[C_k])$, where $C_k$ are are cliques of the graph, and $E[.]$ is an energy defined over the cliques. If $P$ is a Boltzmann distribution over $G$, all local Markov properties will hold. The other way also holds if $P$ is a positive distribution.\n",
    "\n",
    "Let us define a Markov field of binary variables. This will be an Ising model over three nodes. This will contain three cliques of a single node (the on-site fields) and two cliques of two nodes: the edges that connect the nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-19T20:10:30.684603Z",
     "start_time": "2018-11-19T20:10:30.190403Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import dimod\n",
    "\n",
    "n_spins = 3\n",
    "h = {v: 1 for v in range(n_spins)}\n",
    "J = {(0, 1): 2,\n",
    "     (1, 2): -1}\n",
    "model = dimod.BinaryQuadraticModel(h, J, 0.0, dimod.SPIN)\n",
    "sampler = dimod.SimulatedAnnealingSampler()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The probability distribution of a configuration $P(X_1, \\ldots, X_N) = \\frac{1}{Z}\\exp(-\\sum_k E[C_k])$ does not explicitly define the temperature, but it is implicitly there in the constants defining the Hamiltonian. So, for instance, we can scale it by a temperature $T=1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now find out the probability $P(E)$ of each energy level $E$. It can be expressed as a sum over all the states with energy $E$: $P(E)=\\sum_{E(X_1,...,X_n)=E} P(X_1,...,X_N)=\\sum_{E(X_1,...,X_n)=E} \\frac{1}{Z}e^{-E/T}$. The term in the sum is constant (it doesn't depend on $X_1,...,X_N$ anymore). Therefore, we just need to count the number of states such that $E(X_1,...,X_n)=E$. This number is called the *degeneracy* of the energy level $E$, and often noted $g(E)$. Hence, we have $P(E)=\\frac{1}{Z} g(E) e^{-E/T}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's extract this probability for the particular case of our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-19T20:10:32.696067Z",
     "start_time": "2018-11-19T20:10:30.687484Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Degeneracy {-4.0: 1, -2.0: 3, 0.0: 1}\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "temperature = 1\n",
    "response = sampler.sample(model, beta_range=[1/temperature, 1/temperature], num_reads=100)\n",
    "\n",
    "g = {} # dictionary that associate to each energy E the degeneracy g[E]\n",
    "for solution in response.aggregate().data():\n",
    "    if solution.energy in g.keys():\n",
    "        g[solution.energy] += 1\n",
    "    else:\n",
    "        g[solution.energy] = 1\n",
    "print(\"Degeneracy\", g)\n",
    "probabilities = np.array([g[E] * np.exp(-E/temperature) for E in g.keys()])\n",
    "Z = probabilities.sum()\n",
    "probabilities /= Z\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot([E for E in g.keys()], probabilities, linewidth=3)\n",
    "ax.set_xlim(min(g.keys()), max(g.keys()))\n",
    "ax.set_xticks([])\n",
    "ax.set_yticks([])\n",
    "ax.set_xlabel('Energy')\n",
    "ax.set_ylabel('Probability')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, the conditional independences are already encapsulated by the model: for instances, spins 0 and 2 do not interact. In general, it is hard to learn the structure of a probabilistic graphical given a set of observed correlations in the sample $S$. We can only rely on heuristics. The typical way of doing it is to define a scoring function and do some heuristic global optimization. \n",
    "\n",
    "Once we identified or defined the graph structure $G$, we have to learn the probabilities in the graph. We again rely on our sample and its correlations, and use a maximum likelihood or a maximum a posteriori estimate of the corresponding parameters $\\theta_G$ with the likelihood $P(S|\\theta_G)$. This is again a hard problem.\n",
    "\n",
    "Applying the learned model means probabilistic inference to answer queries of the following types:\n",
    "\n",
    "-   Conditional probability: $P(Y|E=e)=\\frac{P(Y,e)}{P(e)}$.\n",
    "\n",
    "-   Maximum a posteriori:\n",
    "    $\\mathrm{argmax}_y P(y|e)=\\mathrm{argmax}_y \\sum_Z P(y, Z|e)$.\n",
    "\n",
    "This problem is in \\#P. Contrast this to deep learning: once the neural network is trained, running a prediction on it is relatively cheap. In the case of probabilistic graphical models, inference remains computationally demanding even after training the model. Instead of solving the inference problem directly, we use approximate inference with sampling, which is primarily done with Monte Carlo methods on a classical computer. These have their own problems of slow burn-in time and correlated samples, and this is exactly the step we can replace by sampling on a quantum computer.\n",
    "\n",
    "For instance, let us do a maximum a posteriori inference on our Ising model. We clamp the first spin to -1 and run simulated annealing for the rest of them to find the optimal configuration. We modify the simulated annealing routine in `dimod` to account for the clamping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-19T20:10:32.992517Z",
     "start_time": "2018-11-19T20:10:32.705659Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from dimod.reference.samplers.simulated_annealing import greedy_coloring\n",
    "\n",
    "clamped_spins = {0: -1}\n",
    "num_sweeps = 1000\n",
    "βs = [1.0 - 1.0*i / (num_sweeps - 1.) for i in range(num_sweeps)]\n",
    "\n",
    "# Set up the adjacency matrix.\n",
    "adj = {n: set() for n in h}\n",
    "for n0, n1 in J:\n",
    "    adj[n0].add(n1)\n",
    "    adj[n1].add(n0)\n",
    "# Use a vertex coloring for the graph and update the nodes by color\n",
    "__, colors = greedy_coloring(adj)\n",
    "\n",
    "spins = {v: np.random.choice((-1, 1)) if v not in clamped_spins else clamped_spins[v]\n",
    "         for v in h}\n",
    "for β in βs:\n",
    "    energy_diff_h = {v: -2 * spins[v] * h[v] for v in h}\n",
    "\n",
    "    # for each color, do updates\n",
    "    for color in colors:\n",
    "        nodes = colors[color]\n",
    "        energy_diff_J = {}\n",
    "        for v0 in nodes:\n",
    "            ediff = 0\n",
    "            for v1 in adj[v0]:\n",
    "                if (v0, v1) in J:\n",
    "                    ediff += spins[v0] * spins[v1] * J[(v0, v1)]\n",
    "                if (v1, v0) in J:\n",
    "                    ediff += spins[v0] * spins[v1] * J[(v1, v0)]\n",
    "\n",
    "            energy_diff_J[v0] = -2. * ediff\n",
    "        for v in filter(lambda x: x not in clamped_spins, nodes):\n",
    "            logp = np.log(np.random.uniform(0, 1))\n",
    "            if logp < -1. * β * (energy_diff_h[v] + energy_diff_J[v]):\n",
    "                spins[v] *= -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running this algorithm, we can obtain the most likely configuration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-19T20:10:33.018780Z",
     "start_time": "2018-11-19T20:10:32.997312Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: -1, 1: -1, 2: 1}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spins"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Boltzmann machines\n",
    "\n",
    "A Boltzmann machine generates samples from a probability distributition $P(\\textbf{v})$ inferred from the data, where $\\textbf{v} \\in \\{0,1\\}^n$. The assumption is that this distribution lies on a latent space that can be paramerized by a set of hidden variables $\\textbf{h} \\in \\{0,1\\}^n$, such that $P(\\textbf{v})=\\sum_h P(\\textbf{v}|\\textbf{h})P(\\textbf{h})$. The joint probability distribution is modeled as a Gibbs distribution with the energy defined by an Ising Model: $P(\\textbf{v}, \\textbf{h})=\\frac{1}{Z} e^{-\\beta E(\\textbf{h},\\textbf{v})}$ and $E(\\textbf{h},\\textbf{v})=-\\sum_{i,j} W_{ij} h_i v_j$. It can then be shown that $p(\\textbf{h}|\\textbf{v})=\\sigma(W \\cdot \\textbf{v})$ and $p(\\textbf{v}|\\textbf{h})=\\sigma(W \\cdot \\textbf{h})$, where $\\sigma$ is the sigmoid function defined by $\\sigma(x)=\\frac{1}{1+e^{-x}}$.\n",
    "\n",
    "To train a Boltzmann machine, we look for the weights $W$ that maximizes the log-likelihood $L=\\sum_{\\textbf{v} \\in S} \\log(p(\\textbf{v}|W))$, where $S$ is the training set. This function can be optimized using regular gradient ascent: $W_{ij}^{(t+1)}=W_{ij}^{(t)} + \\eta \\frac{\\partial L}{\\partial W_{ij}}$. Computing the gradient $\\frac{\\partial L}{\\partial W_{ij}}$ is the hard part. Indeed, we can show that \n",
    "\n",
    "$$\\frac{\\partial L}{\\partial W_{ij}}=\\frac{1}{|S|} \\sum_{\\textbf{v} \\in S} \\mathbb{E}_{\\textbf{h} \\sim P(\\textbf{h}|\\textbf{v})}[h_i v_j] - \\mathbb{E}_{(\\textbf{h},\\textbf{v}) \\sim P(\\textbf{h},\\textbf{v})}[h_i v_j]$$.\n",
    "\n",
    "The first expectation value is easy to compute: it is equal to $\\sigma \\left( \\sum_j W_{ij} v_j \\right) v_j$. We only need to sum those expectation values over the dataset. This is called the positive phase, after its positive sign in the gradient.\n",
    "\n",
    "The second expectation value cannot be simplified as easily, since it is taken over all possible configuration $\\textbf{v}$ and $\\textbf{h}$. It would take an exponential amount of time to compute it exactly. We can use the exact same quantum sampling method as above to outsource this part of the calculation to a quantum processing unit and train Boltzmann machines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "[1] Koller, D., Friedman, N., Getoor, L., Taskar, B. (2007). [Graphical Models in a Nutshell](https://ai.stanford.edu/~koller/Papers/Koller+al:SRL07.pdf). In *Introduction to Statistical Relational Learning*, MIT Press. <a id='1'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 [3.6]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
